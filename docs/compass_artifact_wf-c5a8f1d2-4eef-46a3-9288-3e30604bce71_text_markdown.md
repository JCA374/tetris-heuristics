# Technical Deep-Dive: High-Performance Tetris AI Systems

Classification-Based Modified Policy Iteration (CBMPI) holds the academic record at **51 million lines cleared**, using just 9 hand-crafted features and converging in 6 iterations with 256 million samples—six times more efficient than competing methods. This breakthrough from 2013 remains unbeaten, while modern deep learning approaches struggle to clear even 1,000 lines, demonstrating that well-engineered classical optimization dramatically outperforms neural networks for Tetris by three orders of magnitude. The secret lies in feature engineering: Dellacherie's 6 carefully designed features like "eroded piece cells" and "column transitions" achieve 660,000 lines, while extending to 9 features with hole depth and pattern diversity pushes performance to 35+ million lines. For reproducibility, the winning formula combines Cross-Entropy Method optimization (population size 100, elite fraction 10%, constant noise 4.0) with Dellacherie-Thiery features, requiring approximately one month of training on 2009 hardware or days on modern multi-core systems.

## Elite performance: what actually works at scale

The performance hierarchy reveals a stark reality. **CBMPI achieved 51 million lines** on the standard 20×10 board using classification-based policy iteration with Dellacherie-Thiery features and RBF height encodings. Cross-Entropy Method implementations (Thiery & Scherrer, 2009) reached **35 million lines ± 20%** using 8 features, while CMA-ES matched this with covariance matrix adaptation. These methods dwarf traditional approaches: Dellacherie's hand-crafted heuristic cleared 660,000 lines in the original game and 5.2 million in simplified Tetris, while Böhm's genetic algorithm with 2-piece lookahead achieved an extraordinary **480 million lines**—though this isn't directly comparable due to knowing the next piece.

Deep reinforcement learning tells a different story. The best GitHub implementations (nuno-faria/tetris-ai) achieve approximately 800,000 points translating to roughly 1,000 lines cleared—respectable but **99.998% behind the classical record**. Stanford's CS231n DQN implementation averaged just 230 game length, and a comprehensive 2024 comparative study confirmed that DQN, C51, and PPO all "significantly underperformed" heuristic agents in both score and computational efficiency. The research community concluded that existing solutions "fall far short of what can be achieved by expert players playing without time pressure," and no AI system has surpassed the 2013 CBMPI record despite advances in deep learning.

## Cross-Entropy Method: the reproducible champion

The noisy Cross-Entropy Method (Szita & Lörincz, 2006) provides the most well-documented path to high performance. **Initialize with μ=(0,...,0) and σ²=(100,...,100)** for all weight dimensions. Each iteration generates **N=100 weight vectors** sampled from a multivariate Gaussian distribution. Evaluate each vector across 100 Tetris games (though 1 game per vector works during training), select the **top 10% as elites** (ρ=0.10), then update the mean to the average of elite vectors and variance to their variance plus constant noise Zt=4.0. This noise parameter proves critical—without it, convergence stalls at suboptimal policies clearing only 100-3,000 lines. With constant noise of 4.0, performance reliably reaches 100,000-200,000 lines. Linearly decreasing noise (Zt = max(5 - t/10, 0)) can achieve peak performance of 350,000 lines but introduces high variance across runs.

Run the algorithm for **200 iterations** with 100 games per evaluation, totaling approximately 2 million simulated games. On 2009 hardware this required one month of continuous training; modern multi-core CPUs complete this in days when properly parallelized. The key insight: **feature quality matters infinitely more than algorithm sophistication**. Szita's implementation using 21 Bertsekas features (10 column heights, 9 height differences, maximum height, holes) achieved 350,000 lines. Switching to 8 Dellacherie-Thiery features while keeping the exact same algorithm and hyperparameters yields 35 million lines—a 100× improvement through feature engineering alone.

## Dellacherie features: the foundation of success

Pierre Dellacherie's hand-crafted heuristic (2003) uses six elegantly simple features. **Landing height (l)** measures the row number where the piece lands (weight: -1, minimize). **Eroded piece cells (e)** equals rows eliminated multiplied by cells from the placed piece that were cleared—placing an I-piece that completes 4 lines scores 4×4=16 (weight: +1, maximize). **Row transitions (Δr)** counts horizontal filled↔empty transitions including board edges as filled (weight: -1). **Column transitions (Δc)** counts vertical transitions with board edges and floor as filled (weight: -1). **Holes (L)** are empty cells with at least one filled cell above in the same column (weight: -4, heavily penalized). **Wells (W)** sum the cumulative depth of vertical sequences of empty cells with both neighbors occupied: Σw∈wells(1+2+...+depth(w)) where a well of depth 3 contributes 1+2+3=6 (weight: -1).

The evaluation function **Score = -l + e - Δr - Δc - 4L - W** achieved **660,000 lines average** on original Tetris (56 games) and 5.2 million on simplified Tetris. These weights were hand-tuned through expert intuition. Particle Swarm Optimization (El-Tetris variant) refined the weights to [-4.500158, +3.4181268, -3.2178882, -9.348695, -7.899265, -3.3855972], replacing eroded cells with raw "rows eliminated" and achieving 16 million lines—tripling Dellacherie's performance through weight optimization alone.

Thiery & Scherrer extended Dellacherie with two additional features. **Hole depth (D)** sums the number of filled cells above each hole, punishing deeply buried holes. **Rows with holes (R)** counts rows containing at least one hole (two holes in one row count as 1, not 2). The 8-feature "DU Controller" optimized by Cross-Entropy achieved the breakthrough: **35 million lines with weights [-12.63, +6.60, -9.22, -19.77, -13.08, -10.49, -1.61, -24.04]** for [landing height, eroded cells, row transitions, column transitions, holes, wells, hole depth, rows with holes]. Note that **rows with holes receives the largest penalty (-24.04)**, making it the most impactful discovery. Pattern diversity adds a ninth feature measuring distinct patterns in height differences (magnitude \u003c3) between adjacent columns, though its contribution appears marginal.

## CBMPI: the unbeaten record holder

Classification-Based Modified Policy Iteration combines policy approximation through classification with value function approximation through regression—a hybrid approach that exploits the insight that **good Tetris policies are easier to represent than accurate value functions**. The algorithm alternates between an evaluation step (vₖ = (Tπₖ)ᵐvₖ₋₁ + εₖ) applying m Bellman operators to approximate the value function, and a greedy step (πₖ₊₁ = G[(Tπₖ)ᵐvₖ₋₁]) computing a greedy policy via classification. The optimal rollout length m=2 for CBMPI balances bias-variance tradeoff, while traditional Direct Policy Iteration needs m=5-10.

The regressor uses **linear function approximation v(s) = φ(s)ᵀw** with 15 features: 9 Dellacherie-Thiery features plus 5 RBF (Radial Basis Functions) for height encoding. RBFs take the form **exp(-|c-ih/4|²/(2(h/5)²))** for i=0,1,2,3,4 where c equals average column height and h equals board height (10 or 20). These Gaussian bumps centered at different height intervals help the value function capture non-linear relationships. Weights initialize at w=0 and update via standard least-squares regression on sampled states.

The classifier represents the policy as **πᵤ(s) = argmaxₐ ψ(s,a)ᵀu** optimized via CMA-ES (Covariance Matrix Adaptation Evolution Strategy) with ρ=0.5 selection proportion, η=0 noise, and population size proportional to feature dimensionality. Each iteration samples **N=84,000 states** for the 10×10 board or uses a budget-constrained approach for 20×10. The critical innovation: subsample the training distribution to ensure uniform height representation, preventing the algorithm from learning only on nearly-empty boards.

CBMPI converges in **2-10 iterations** for small boards and **6 iterations** for the standard 20×10 board. Total sample budget: approximately **256 million samples** compared to Cross-Entropy's 1.7 billion—a **6× improvement in sample efficiency**. This translated to the **world record of 51 million lines** on the large board (averaged over 10,000 games) and 5,000 lines on the small 10×10 board. The algorithm's effectiveness stems from recognizing that classification (learning "which action is best") proves fundamentally easier than regression (learning "exactly how good is this state") in Tetris's complex state space.

## Evolutionary algorithms and genetic approaches

CMA-ES (Covariance Matrix Adaptation Evolution Strategy) matched Cross-Entropy's 35 million line performance while offering theoretical advantages in maintaining a full covariance matrix rather than diagonal variance. Standard CMA-ES parameters: population size **λ = 4 + ⌊3ln(n)⌋** where n equals dimensionality, parent count **μ = λ/2**, and sophisticated step-size adaptation through evolution path tracking. Applied to Tetris with the BCTS feature set, CMA-ES essentially "rediscovered" Dellacherie's strategy—the optimized weights closely matched his hand-tuned values, providing independent validation of the feature space's naturalness.

Genetic algorithms show tremendous promise with proper configuration. Böhm, Kókai & Mandl (2005) tested three rating functions across 11 features including pile height, connected holes, height difference (max-min), maximum well depth, landing height, occupied squares weighted by height, and transitions. Their **linear rating function Rl(b,ω) = Σωᵢ·rᵢ(b)** with 2-piece lookahead achieved the extraordinary **480 million lines**. The winning weight vector: ω = (-62709, -30271, 0, -48621, 35395, -12, -43810, 0, 0, -4041, -44262, -5832). However, 2-piece lookahead makes this incomparable to standard 1-piece benchmarks—knowing the next piece provides several orders of magnitude advantage.

Typical genetic algorithm configuration: **population size 50-100**, tournament selection keeping top 50%, crossover rate 0.7-0.95, mutation rate 0.05-0.2 with mutation step 0.2, running 10-50 generations with elite preservation of top 1-2 individuals. Fitness evaluation requires 5-12 games per individual with smoothing that ignores best and worst games when sample size ≥7. A successful 4-feature implementation (aggregate height, complete lines, holes, bumpiness) achieved 2.18 million lines over two weeks of continuous play with GA-optimized weights [-0.510066, +0.760666, -0.35663, -0.184483].

## Deep learning: the persistent underperformer

Deep Q-Networks for Tetris consistently fail to approach classical method performance despite extensive tuning. The typical successful architecture uses **2 hidden layers with 32 neurons each**, ReLU activations, and linear output. Input: 4-9 hand-crafted features (holes, bumpiness, aggregate height, lines cleared). Hyperparameters that work reasonably well: **learning rate 0.001** (Adam optimizer), **discount factor γ=0.95-0.99**, **epsilon 1.0 decaying to 0.0 over 75% of episodes**, **replay buffer 20,000 experiences**, **batch size 512**, and **2,000-20,000 training episodes**. This configuration, exemplified by nuno-faria/tetris-ai on GitHub, achieves approximately 800,000 points—the best documented deep learning result but still three orders of magnitude below classical methods.

Stanford's CS231n project (2016) explored convolutional architectures for raw pixel input. Their best column-based architecture: conv3-32 → conv3-32 → conv3-64 → column collapse (reduces each 10×20 column to single pixel maintaining 64 channels) → conv3-128 → conv1-128 → conv3-128 → FC-128 → FC-512 → FC-13 output. Batch normalization and dropout (0.75 retention probability) improved stability. They used **RMSProp optimizer** with learning rate **2×10⁻⁶ manually annealed to 5×10⁻⁷**, squared gradient momentum 0.95, and discount γ=0.75-0.9. Despite sophistication, average game length reached only 230 moves—far below heuristic baselines.

The key failure mode: **sparse reward problem**. Random actions almost never clear lines, providing no learning signal. Prioritized experience replay helps by sampling transitions proportional to TD error: priority = |Q(s,a) - [r + γ·max Q'(s',a')]|. Stanford found this smoothed action histogram distribution and improved convergence, though at significant computational cost recalculating priorities each epoch. Transfer learning offers another approach: pre-train on a heuristic reward function (Fitness = -0.51×Height + 0.76×Lines - 0.36×Holes - 0.18×Bumpiness), then switch to game score. This causes initial performance drop but enables faster recovery than training from scratch—their transfer learning model averaged 2.1 lines versus 0.1 lines training from scratch.

**Action representation critically impacts learning**. The "grouped actions" approach—where each action represents a final column placement and rotation rather than individual button presses (left, right, rotate, drop)—reduces state complexity by approximately 1/7 and dramatically accelerates convergence. Stanford reported 85% improvement switching from 13 individual actions to grouped placement decisions. The network evaluates all valid placements (typically 10 columns × 4 rotations = 40 positions) and selects the highest Q-value.

Advanced techniques like Double DQN (separate networks for action selection and evaluation to reduce overestimation bias) and Dueling Networks (separate streams for state value V(s) and action advantage A(s,a)) appear in literature but show limited practical improvement for Tetris. The fundamental challenge remains: **policy space proves far easier to optimize than value function space** in this domain, explaining why policy-search methods (CEM, genetic algorithms, CBMPI) so dramatically outperform value-based deep learning.

## Training procedures and computational realities

Cross-Entropy Method simulation requirements reveal the scale involved. Each iteration generates **100 weight vectors**, evaluates each on **100 games**, across **200 iterations**—totaling approximately **2 million games**. Each game in a well-performing controller might last millions of pieces, taking hours or days to complete. Szita & Lörincz (2006) reported training times exceeding one month on 2006 hardware. Thiery & Scherrer (2009) optimized to 50,000-100,000 moves per second on a desktop CPU but still required roughly one month per feature set configuration. Modern implementations on AWS EC2 c5a.16xlarge instances (64 vCPUs, 131GB RAM) parallelize across cores, reducing wall-clock time to days or weeks.

CBMPI's sample budget for the standard 20×10 board: **16 million samples per iteration × 6 iterations = 96 million samples** total to convergence, though the paper reports up to 256 million samples for thorough evaluation. Each "sample" represents a state-action pair with m-step rollout, requiring simulation of m+1 pieces. With rollout length m=2 and evaluating all ~40 actions per state, the budget B = (m+1)NM|A| = 3 × N × 1 × 40 where N equals the number of states sampled (approximately 133,000 per iteration given B=16M). The algorithm converges much faster than Cross-Entropy—**6 iterations versus 60+**—but each iteration involves sophisticated CMA-ES optimization of the classifier that may take comparable wall-clock time to CE's simpler Gaussian updates.

Deep learning training proves more accessible computationally but less effective in outcomes. A typical DQN run: **2,000 episodes** with each episode potentially thousands of moves early in training or very short when performance is poor. At 512 batch size with 20,000-experience replay buffers, gradient updates occur every move after the initial buffer fill. Training duration: **2-4 hours on CPU** for basic implementations, though longer runs (20,000 episodes) may take days. The nuno-faria implementation terminated at episode 1,460 (of 2,000) because the agent played so long that training became impractical—an ironic success problem. GPU acceleration provides approximately 3× speedup. JAX backend shows up to 20× faster training/predict operations than TensorFlow in some reports, though framework choice matters less than fundamental architectural decisions.

Convergence criteria vary by method. Cross-Entropy typically uses **fixed iteration counts (80-200)** since the algorithm monotonically improves on average. Alternative stopping conditions: variance threshold (σ² \u003c ε for all dimensions), performance plateau (no improvement over k iterations), or computational budget exhaustion. Monitoring logs every iteration: mean population fitness, best fitness, elite mean (next μ), variance per dimension, and test performance (30-100 games with current μ) to track progress. CBMPI uses iteration counts as the primary criterion, typically converging in 2-10 iterations for small boards and 6 for large boards. Deep learning employs episode counts, performance targets, or early stopping when validation performance plateaus.

## Feature engineering: the decisive factor

The performance gap between feature sets dwarfs algorithmic differences. Bertsekas features (22 dimensions: 10 column heights h₁...h₁₀, 9 height differences Δhᵢ = |hᵢ - hᵢ₊₁|, maximum height H, holes L) achieve **350,000 lines** with Cross-Entropy. Dellacherie features (6 dimensions: landing height, eroded cells, row/column transitions, holes, wells) achieve **660,000 lines** hand-tuned and **35 million lines** when optimized with the same Cross-Entropy algorithm. This **100× improvement** stems purely from better features capturing what actually matters in Tetris strategy.

**Hole count (L)** consistently receives heavy penalties across all successful controllers. Dellacherie weighted it at -4 (quadruple the penalty of other features), CEM-optimized weights show -13.08, and El-Tetris found -7.899265. The intuition: holes beneath filled cells become exponentially harder to clear as more pieces pile above them, leading to rapid game termination. **Hole depth (D)** extends this by summing filled cells above each hole—a hole buried under 10 blocks is catastrophically worse than one under 2 blocks. **Rows with holes (R)** provides complementary information, counting how many rows contain at least one hole regardless of total hole count. CEM assigned this the largest magnitude weight at -24.04 in the DU controller, suggesting it may be the single most important Tetris feature ever discovered.

**Column transitions (Δc)** and **row transitions (Δr)** measure surface roughness. Transitions count filled↔empty boundaries including board edges as filled. Smooth surfaces (few transitions) allow flexible piece placement; rough, jagged surfaces severely constrain options. Dellacherie weighted both at -1; CEM optimization found column transitions far more critical at -19.77 versus row transitions at -9.22—approximately double the penalty. This makes intuitive sense: vertical discontinuities (column transitions) create the problematic holes, while horizontal discontinuities (row transitions) indicate line completion opportunities.

**Landing height (l)** and **eroded piece cells (e)** form a complementary pair. Landing height (weight: -12.63 in DU controller) penalizes building tall structures that risk game termination when the stack reaches the top. Eroded piece cells (weight: +6.60) rewards efficient line clearing, especially the coveted "Tetris" move clearing 4 lines with an I-piece (scores 4×4=16). The asymmetric weights (roughly 2:1 ratio) suggest that avoiding height matters roughly twice as much as maximizing clears in long-term survival.

**Wells (W)** capture a subtle strategic element: sequences of empty cells with both neighbors occupied, scored as Σw∈wells(1+2+...+depth(w)). A well of depth 3 contributes 1+2+3=6 to the feature value. Dellacherie weighted this at -1; CEM found -10.49. Deep wells create placement constraints—only I-pieces fit cleanly into a 4-deep well, and mis-filling wells creates hard-to-clear holes. The cumulative depth formula (triangular numbers) appropriately penalizes deep wells exponentially worse than shallow ones.

RBF (Radial Basis Functions) height features in CBMPI provide non-linear encoding of board state: **exp(-|c-ih/4|²/(2(h/5)²))** for i=0,1,2,3,4 where c represents mean column height. These five Gaussian bumps centered at different heights (0, h/4, h/2, 3h/4, h) activate most strongly when the board mean height matches their center. This encoding allows the value function to learn height-dependent strategies—for example, playing more conservatively as the board fills. Combined with the 9 Dellacherie-Thiery features plus a constant bias term, this creates a 15-dimensional feature space that achieved the world-record performance.

## Implementation guide for reproduction

**Minimal working Cross-Entropy Method** (expect 100k-1M lines): Clone corentinpla/Learning-Tetris-Using-the-Noisy-Cross-Entropy-Method from GitHub. Install Python 3.7+ with NumPy and SciPy. Configure: population_size=50, elite_fraction=0.1, noise=4.0 (constant), iterations=50, games_per_eval=10, features=Dellacherie 6-feature set. Initialize μ=zeros(6), σ²=ones(6)*100. Run CE_method_with_noise.py. Training time: hours to days on modern CPU. Expected performance: hundreds of thousands of lines.

**State-of-the-art Cross-Entropy** (target 35M lines): Use population_size=100, elite_fraction=0.1, noise=4.0, iterations=200, games_per_eval=100, features=Dellacherie-Thiery 8-feature set (add hole depth and rows with holes). Parallelize game evaluation across CPU cores—critical for reasonable wall-clock time. Implement efficient board representation as int8_t[20][10] array with 0=empty, \u003e0=filled. Optimize move generation to check all rotations×columns efficiently—typically 20-40 valid placements per piece. Expected training time: 1-4 weeks on multi-core system. Expected performance: 35M ± 20% lines.

**Functional deep learning baseline** (expect hundreds of lines): Clone nuno-faria/tetris-ai. Install Python 3.9+ with Keras 3.5.0, TensorFlow/JAX/PyTorch backend (JAX recommended for speed), NumPy, OpenCV, Pillow. Use provided sample.keras pre-trained model for immediate testing or train from scratch. Architecture: 2 hidden layers, 32 neurons each, ReLU activation, linear output. Features: [lines_cleared, holes, bumpiness, total_height]. Hyperparameters: lr=0.001 (Adam), γ=0.95, ε: 1.0→0.0 over 1500 episodes, replay_buffer=20k, batch_size=512, max_episodes=2000. Run: `python3 run.py --train`. Monitor: `tensorboard --logdir ./logs`. Training time: 2-4 hours CPU, 1-2 hours GPU. Expected: 800k points (~1000 lines).

**Game simulation essentials**: Standard board: 10 columns × 20 rows. Seven standard tetrominoes (I, O, T, S, Z, J, L) with Super Rotation System. Simplified Tetris (used in most research): piece only appears after placement decision made—game over when placement causes board overflow above row 20. Original Tetris: piece spawns at top center and must fit—game over when piece cannot spawn. Simplified variant allows approximately 3× higher scores due to relaxed termination conditions. Random piece generation (fully random) versus modern bag system (7-permutation) affects strategy and scores significantly—specify which variant for reproducibility.

**Critical implementation details**: Move generation must enumerate all valid final placements. For each rotation (0-3): `rotated = rotate_piece(piece, rotation)`, then for each column (0 to board_width - piece_width): check `is_valid_placement(rotated, column, board)`. Valid moves: typically 20-40 per piece depending on rotation symmetries (O-piece has 1 rotation, I-piece has 2, others have 4). Feature calculation timing matters—some implementations calculate features before line clearing, others after. Szita extracted features from the board state resulting from the move but before clearing completed lines, while others calculate after clearing. Both work but produce different feature values; document which approach you use.

## GitHub repositories and frameworks

**Top-tier implementations for study**:

**nuno-faria/tetris-ai** (production-quality DQN): Python, Keras 3.5.0 with TensorFlow/JAX/PyTorch backends. Includes trained model (sample.keras) achieving 800k+ points. Clean codebase with tensorboard integration. Best starting point for deep learning approaches. Command: `python3 run.py --train` for training, `--test` for evaluation. Dependencies: keras, numpy, opencv-python, pillow, tqdm, tensorboard. GPU recommended but not required.

**hrpan/tetris_mcts** (advanced research): Monte Carlo Tree Search with Temporal Difference Learning inspired by AlphaGo. Achieved 5,678 lines with 1000 simulations per move—most sophisticated approach generalizable beyond Tetris. Uses PyTetris environment with pybind11. Includes Docker support. Command: `python play.py --agent_type ValueSimLP --online --ngames 1000 --mcts_sims 100`. Requires compilation of C++ components. Suitable for researchers exploring MCTS/AlphaZero-style approaches.

**corentinpla/Learning-Tetris-Using-the-Noisy-Cross-Entropy-Method** (reference CEM): Pure Python implementation of Szita & Lörincz method. Files: CE_method.py (basic), CE_method_with_noise.py (full algorithm). Includes extension to Thiery features. Well-documented with clear variable names. Best starting point for understanding and reproducing classical Cross-Entropy results. No heavy dependencies—NumPy and basic Python libraries.

**fischly/tetris-ai** (double DQN with CNN): PyTorch implementation in Jupyter notebooks. Includes two pre-trained models (good-cnn-1.pt, good-cnn-2.pt) and professional replay data (250MB). Notebooks: dqn.ipynb for training, run-renderer.ipynb for visualization. Uses wandb for experiment tracking. Genetic algorithm approach in separate branch. Good for interactive exploration and understanding DQN training dynamics. Tested on Ubuntu 22.04.

**vietnh1009/Tetris-deep-Q-learning-pytorch** (clean PyTorch DQN): Simple, well-structured PyTorch implementation. Pre-trained models in trained_models/tetris/ directory. State representation: [lines, holes, total_bumpiness, sum_height]. Commands: `python train.py` and `python test.py`. Python 3.6+, minimal dependencies (torch, numpy, cv2, PIL, matplotlib). Excellent for learning DQN implementation details with readable code.

**Game environments**: Kautenja/gym-tetris (NES Tetris for OpenAI Gym, install: `pip install gym-tetris`), Max-We/Tetris-Gymnasium (modern Gymnasium-compatible, fully configurable), tristanrussell/gym-simpletetris (pure Python, `pip install gym-simpletetris`), jaybutera/tetrisRL (`pip install tetrisrl`). For research-grade implementations, gym-tetris provides authentic NES experience with 256 discrete actions (reducible to 20 or 6 with wrappers), while Tetris-Gymnasium offers maximum configurability and modern API compatibility.

**Framework ecosystem**: TensorFlow/Keras (most common, stable, extensive documentation), PyTorch (increasingly popular, dynamic graphs, preferred by researchers), JAX (emerging, reports 20× faster training than TensorFlow in some implementations), Stable-Baselines3 (high-level RL library with PPO/DQN/SAC, requires Python 3.9+, PyTorch backend). Supporting libraries: NumPy (universal), OpenCV (cv2 for image processing), Pygame (rendering), Pillow (image manipulation), Tqdm (progress tracking), Tensorboard/Wandb (training monitoring), Pandas (data management), Matplotlib (visualization).

## Performance metrics and validation

Testing rigor critically affects result reliability. **Small board (10×10)**: Average over **200-10,000 games** depending on precision requirements. Typical: 200 games for rough assessment, 10,000 for precise comparison. **Standard board (20×10)**: Average over **20-200 games**—fewer games needed because high-performing controllers run for days per game. **Training**: 100 games per parameter evaluation standard in genetic algorithms; Cross-Entropy uses 1 game per vector during optimization, 100 games for final evaluation. Multiple independent runs (typically 100 trials) required for statistical significance given high variance in Tetris outcomes.

Performance metrics hierarchy: **Lines cleared** serves as the primary metric—most comparable across implementations. **Game score** depends heavily on scoring rules (modern Tetris awards exponentially more points for multi-line clears; simplified research versions use different formulas). **Game length** (pieces placed) correlates with lines cleared but depends on piece distribution. **Success rate** (games reaching certain thresholds) useful for binary classification of competent versus incompetent agents. Secondary metrics: standard deviation (consistency measure—low variance indicates robust strategy), maximum score (best single game—indicates ceiling performance), convergence speed (training efficiency), sample efficiency (total simulator calls to reach performance target).

Benchmark performance ladder from comprehensive comparative studies: **Random agent** averages ~0 lines and 110-230 game length depending on implementation. **Basic heuristic** (simple weighted features) achieves 100-1,000 lines and can play indefinitely with good tuning. **Traditional RL** (λ-PI, fitted Q-iteration) reaches 400-6,800 lines. **Modern deep RL** (DQN, C51, PPO) typically achieves 400-1,000 lines. **Good handcrafted heuristics** (Dellacherie) clear 660,000 lines. **Optimized evolutionary methods** (Cross-Entropy, CMA-ES) achieve 35 million lines. **State-of-the-art ADP** (CBMPI) holds the record at 51 million lines.

The gap between methods spans **four orders of magnitude** from traditional RL (~5k lines) to CBMPI (51M lines). The 2024 comparative study's conclusion resonates: "Well-designed heuristics demonstrate advantages in specific problem domains" over reinforcement learning. Expert human players—when given unlimited time to plan each move—still outperform all AI systems, though quantitative human benchmarks remain unpublished in academic literature. This human superiority suggests current AI approaches miss critical strategic elements that expert players employ, such as multi-move planning for "Tetris" setups (clearing 4 lines with an I-piece) and sophisticated pattern recognition.

## Computational efficiency and practical deployment

Sample efficiency hierarchy reveals the cost of different approaches. **CBMPI** uses approximately **256 million samples** to convergence (6 iterations × ~40M samples per iteration), achieving 51 million lines. **Cross-Entropy Method** requires approximately **1.7 billion samples** (200 iterations × 100 vectors × 100 games × average moves per game) for 35 million lines—making CBMPI **6× more sample efficient** for equivalent performance. **Genetic algorithms** vary widely but typically need hundreds of millions of samples across generations. **Deep reinforcement learning** proves least efficient, requiring tens of millions of state transitions to reach even 1,000 lines cleared—roughly 10,000× less sample efficient than CBMPI per unit of final performance.

Training time dependencies: CEM and CBMPI training duration depends primarily on game simulation speed and how long successful controllers play. Early iterations complete quickly (poor controllers die fast), but late-stage evaluation games can run for days—a controller averaging 35 million lines might take **days per game** at typical simulation speeds. Parallelization proves essential: Thiery & Scherrer achieved 50,000-100,000 moves per second on 2009 desktop CPUs; modern multi-core implementations on 64-core cloud instances scale near-linearly with core count for population evaluation. Wall-clock training time: approximately **one month** on 2009 single-core hardware, **days to one week** on modern multi-core systems with proper parallelization.

Deep learning training proves more predictable: **2-4 hours on CPU** for basic DQN reaching hundreds of lines cleared, **1-2 hours on GPU** with comparable results. Longer runs (20,000 episodes) extend to days but show diminishing returns—the sparse reward problem fundamentally limits deep RL effectiveness in Tetris. JAX backend reports up to 20× faster training/prediction than TensorFlow in some implementations, suggesting framework choice significantly impacts iteration speed. Batch size affects GPU utilization—**512 proves optimal** across multiple implementations, balancing memory usage and computational efficiency.

Inference speed strongly favors classical methods. **Heuristic controllers** evaluate moves nearly instantaneously: calculate 6-9 features for each of ~40 possible placements (microseconds per feature extraction), compute weighted sums, select maximum. Total decision time: **milliseconds**. Deep neural networks require forward passes through multiple layers—still fast (milliseconds to tens of milliseconds) but 10-100× slower than heuristic evaluation. For real-time gameplay at 60 FPS with pieces falling over multiple frames, all methods prove fast enough. For high-speed simulation (millions of games for training), heuristic evaluation speed advantages compound dramatically.

Memory requirements: Board state requires minimal storage (10×20 boolean array or 200 bytes). Deep RL replay buffers dominate memory usage: **20,000 experiences** at ~1KB each (state, action, reward, next_state) equals approximately 20MB. Network parameters: modern Tetris DNNs typically **1-50 MB** depending on architecture. CEM/CBMPI store population vectors: **100 vectors × 8 features × 8 bytes = 6.4KB**—negligible. Training hardware: CPU sufficient for classical methods (highly parallelizable across cores), GPU strongly recommended for deep learning (3× speedup reported), RAM requirements modest (4-8GB typical).

Deployment considerations favor heuristics for production systems. **Model size**: CEM produces 8 floating-point weights (64 bytes) versus DQN's 1-50MB neural network. **Inference dependencies**: Heuristics need zero external libraries (pure calculation) versus DQN requiring TensorFlow/PyTorch runtime (100s of MB). **Reproducibility**: Heuristic evaluation perfectly deterministic; neural networks show minor floating-point variations across hardware/libraries. **Interpretability**: Feature weights directly show strategy (e.g., "rows with holes" weighted at -24.04 clearly indicates priority) versus neural network black-box. **Performance**: Heuristics clear 35-51M lines; deep learning struggles to clear 1,000.

## Key lessons and future directions

The Tetris AI landscape reveals a counterintuitive lesson: **classical optimization with hand-crafted features dominates deep learning by three orders of magnitude** despite deep learning's success in domains like vision and language. The explanation lies in problem structure. Tetris has a relatively small set of critical strategic principles (avoid holes, minimize height, maintain smooth surfaces, clear lines efficiently) that expert humans can articulate and translate into features. Once these features exist, the optimization problem—finding good weights—proves tractable for evolutionary algorithms. In contrast, learning both feature extraction and decision policy end-to-end from sparse rewards overwhelms current deep RL methods.

Feature engineering decides everything. Dellacherie's insight that **eroded piece cells** (rewarding pieces that participate in line clears) and **rows with holes** (heavily penalizing board deterioration) capture essential Tetris strategy enabled performance jumps from thousands to millions of lines. The transition from Bertsekas features (columns heights, differences) to Dellacherie features (strategic concepts like transitions, wells, eroded cells) represents a **100× performance leap** with identical algorithms. This suggests Tetris AI progress requires domain expertise and strategic insight more than algorithmic sophistication.

Sample efficiency improvements drove recent progress. CBMPI's 6× sample efficiency advantage over Cross-Entropy Method comes from recognizing that **learning "which action is best" (classification) proves easier than learning "how good is this state" (regression)** in Tetris. This architectural insight—separating policy learning from value learning and using different approximation methods for each—generalizes beyond Tetris to any domain where policy space has simpler structure than value space. The technique: optimize the classifier (policy) using CMA-ES for cost-sensitive classification, while simultaneously fitting a value function with least-squares regression on sampled rollouts.

Deep learning challenges persist. Raw pixel approaches (convolutional networks on board images) consistently underperform feature-based methods. The Stanford CS231n team's sophisticated column-based CNN achieved only 230 average game length despite batch normalization, dropout, prioritized experience replay, and transfer learning. Feature-based DQN (taking hand-crafted features as input) reaches 800k points—respectable but still 99.998% behind CBMPI. The fundamental problem: **sparse rewards** (random actions almost never clear lines) combined with **delayed consequences** (moves create holes that cause death dozens of pieces later) exceed current deep RL methods' credit assignment capabilities without extensive hand-crafting.

Future research directions span multiple approaches. **Automated feature discovery** through genetic programming or neural architecture search might find features humans miss—though Dellacherie-Thiery features already capture strategic principles so well that improvement seems difficult. **Model-based RL** learning a world model to plan ahead could potentially address sparse rewards by simulating outcomes internally. **Hierarchical RL** learning multi-step strategies (e.g., "set up a Tetris opportunity" as a subgoal) might capture the long-term planning expert humans employ. **Meta-learning and hyperparameter optimization** could automatically tune population sizes, noise levels, and learning rates—though this seems marginal given that hyperparameters matter far less than feature quality.

The 2013 CBMPI record remains unbeaten for over a decade despite rapid deep learning progress in other domains. This stability suggests either: (1) Tetris represents a local maximum for current AI techniques where 51 million lines approaches theoretical limits given piece randomness, or (2) fundamental architectural innovations beyond current paradigms will be necessary for further progress. The gap between AI systems (51M lines) and indefinite human play suggests headroom remains. Whether that gap closes through better features, better algorithms, or fundamentally new approaches to sequential decision-making remains an open question driving continued research interest in this decades-old game.

## Sources and further reading

Academic foundations: Szita & Lörincz (2006) "Learning Tetris Using the Noisy Cross-Entropy Method" in Neural Computation 18(12):2936-2941 introduced noise as critical for avoiding local optima in evolutionary methods. Thiery & Scherrer (2009) "Improvements on Learning Tetris with Cross Entropy" achieved the 35M line breakthrough and won the 2008 RL Competition. Gabillon et al. (2013) "Approximate Dynamic Programming Finally Performs Well in the Game of Tetris" at NeurIPS established the current 51M line record with CBMPI. Scherrer et al. (2015) "Approximate Modified Policy Iteration and its Application to the Game of Tetris" in JMLR provided comprehensive theoretical analysis. Algorta & Şimşek (2019) "The Game of Tetris in Machine Learning" arXiv:1905.01652 offers an excellent comprehensive survey of methods through 2019.

Implementation references: Dellacherie's hand-crafted heuristic documented at colinfahey.com/tetris/tetris.html by Colin Fahey. Böhm, Kókai & Mandl (2005) "An Evolutionary Approach to Tetris" demonstrated genetic algorithms achieving 480M lines with 2-piece lookahead. Boumaza (2009) "On the Evolution of Artificial Tetris Players" showed CMA-ES matching Cross-Entropy performance. Stevens & Pradhan (2016) Stanford CS231n report "Playing Tetris with Deep Reinforcement Learning" explored CNNs and transfer learning for Tetris. Multiple 2024 studies including comparative analyses on ScienceDirect confirmed heuristics still dramatically outperform modern deep RL.

Code repositories documented throughout this report provide working implementations: nuno-faria/tetris-ai for deep learning, hrpan/tetris_mcts for MCTS approaches, corentinpla/Learning-Tetris-Using-the-Noisy-Cross-Entropy-Method for classical CEM, fischly/tetris-ai and vietnh1009/Tetris-deep-Q-learning-pytorch for PyTorch DQN, fthomasmorel/Tetris-AI for genetic algorithms. Environment packages: Kautenja/gym-tetris for NES Tetris, Max-We/Tetris-Gymnasium for modern Gymnasium compatibility, tristanrussell/gym-simpletetris for pure Python simplicity. These resources provide complete starting points for reproducing results and conducting further research.